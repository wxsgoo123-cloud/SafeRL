{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0077bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gpytorch\n",
    "from scipy.linalg import solve\n",
    "from gpytorch.kernels import RBFKernel,ScaleKernel,LinearKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00ae169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ed42ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_episodes = 2000\n",
    "max_steps_per_episode = 200\n",
    "test_episodes = 100\n",
    "max_steps_per_test_episode=200\n",
    "CI_parameter= 0.3\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.02\n",
    "epsilon_decay_rate = 0.9985\n",
    "gamma = 0.99\n",
    "lr = 0.0025\n",
    "buffer_size = 50000\n",
    "buffer = deque(maxlen=buffer_size)\n",
    "batch_size = 100\n",
    "update_frequency = 10\n",
    "nrow=20\n",
    "ncol=20\n",
    "sigma_noise=0.1\n",
    "CI_start=0.4\n",
    "CI_end=0.2\n",
    "CI_decay_rate = 0.999\n",
    "#  CI_parameter=0.5\n",
    "# CI_parameter=0.5\n",
    "penalty=60\n",
    "#GP discretization bound\n",
    "Position_lb=-4.8\n",
    "Position_ub=4.8\n",
    "\n",
    "Velocity_lb=-10000\n",
    "Velocity_ub=10000\n",
    "Pole_Angle_ld= -0.418 \n",
    "Pole_Angle_rd= 0.418 \n",
    "\n",
    "\n",
    "\n",
    "Pole_Angle_Velocity_lb= -10000\n",
    "Pole_Angle_Velocity_ub=10000\n",
    "\n",
    "\n",
    "\n",
    "Position_safe_lb=-1.9\n",
    "Position_safe_ub=1.9\n",
    "Pole_Angle_safe_ld= -0.15\n",
    "Pole_Angle_safe_ud=   0.15\n",
    "\n",
    "\n",
    "\n",
    "safety_bound=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00055f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f40539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "    # Initialize the DQN agent\n",
    "    def __init__(self, state_size, action_size, seed, lr):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > 64:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, gamma=0.99)\n",
    "\n",
    "    # Choose an action based on the current state\n",
    "    def act(self, state, eps=0.):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_tensor)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        if np.random.random() > eps:\n",
    "            return action_values.argmax(dim=1).item()\n",
    "        else:\n",
    "            return np.random.randint(self.action_size)\n",
    "        \n",
    "    # Choose an safe action based on the current state and prediction\n",
    "    def act_safe(self, state, model, CI_para, eps=0):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state_tensor)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        possible_action=np.ones(self.action_size)\n",
    "        for i in range(self.action_size):\n",
    "            X=np.concatenate((state, np.array(i).reshape(1, )), axis=0)\n",
    "            X_tensor=torch.from_numpy(X).float().unsqueeze(0).to(device)\n",
    "            mean, log_var = model(X_tensor)\n",
    "            ub= mean.item()+CI_para*np.sqrt(np.exp(log_var.item()))\n",
    "            if ub>=safety_bound:\n",
    "                possible_action[i]=0 \n",
    "                # action_values[0][i]=-1\n",
    "               \n",
    "        if np.random.random() > eps:\n",
    "            return action_values.argmax(dim=1).item()\n",
    "        else:\n",
    "\n",
    "            if np.sum(possible_action)==0:\n",
    "                return action_values.argmax(dim=1).item()\n",
    "            else:\n",
    "                a=np.random.choice(self.action_size,size=1,p=possible_action/np.sum(possible_action))[0]\n",
    "                return int(a)\n",
    "            # return np.random.randint(self.action_size)\n",
    "    # Learn from batch of experiences\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "            \n",
    "def cal_cost(state,action,next_state):\n",
    "    cost1=0\n",
    "    cost2=0\n",
    "    if Position_safe_lb >=next_state[0]:\n",
    "        cost1=Position_safe_lb-next_state[0]\n",
    "    if Position_safe_ub <=next_state[0]:\n",
    "        cost1=next_state[0]-Position_safe_ub\n",
    "    if next_state[2]<=Pole_Angle_safe_ld:\n",
    "        cost2=Pole_Angle_safe_ld-next_state[2]\n",
    "    if next_state[2]>=Pole_Angle_safe_ud:\n",
    "        cost2=next_state[2]-Pole_Angle_safe_ud\n",
    "    return np.maximum(cost1,10*cost2)\n",
    "       \n",
    "\n",
    "class ProbabilisticRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, 1)\n",
    "        self.log_var_head = nn.Linear(hidden_dim, 1)  # log variance for stability\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        mean = self.mean_head(h)\n",
    "        log_var = self.log_var_head(h)\n",
    "        return mean, log_var  # we return log variance to keep it numerically stable\n",
    "    \n",
    "def gaussian_nll_loss(y_pred_mean, y_pred_log_var, y_true):\n",
    "    # NLL for a Gaussian: log(sigma^2) + (y - mu)^2 / sigma^2\n",
    "    loss = 0.5 * (y_pred_log_var + ((y_true - y_pred_mean) ** 2) / torch.exp(y_pred_log_var))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c206feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(new_agent,model):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    episode_rewards = []\n",
    "    test_epi_length=np.zeros(test_episodes)\n",
    "    test_cost_epi=np.zeros(test_episodes)\n",
    "    test_vio_epi=np.zeros(test_episodes)\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        for step in range(max_steps_per_test_episode):\n",
    "            action = new_agent.act_safe(state, model,CI_para=0.7,eps=0)\n",
    "            # action = new_agent.act(state,eps=0)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            cost=cal_cost(state,action,next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            test_epi_length[episode]+=1\n",
    "            test_cost_epi[episode]+=cost\n",
    "            if cost>safety_bound:\n",
    "                test_vio_epi[episode]+=1\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    average_reward = sum(episode_rewards) / test_episodes\n",
    "    print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")\n",
    "    return average_reward,test_cost_epi,test_vio_epi,test_epi_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_rep():\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    # Initialize the DQNAgent\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "    new_agent = DQNAgent(input_dim, output_dim, seed=170715, lr = lr)\n",
    "    #Initialize the Prediction NN\n",
    "    model = ProbabilisticRegressor(input_dim=5, hidden_dim=128)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    iteration = np.zeros(num_episodes)\n",
    "    violation=np.zeros(num_episodes)\n",
    "    cost_episodes=np.zeros(num_episodes)\n",
    "    for episode in range(num_episodes): #Gym versions >= 0.26 where env.reset() returns a tuple (obs, info)\n",
    "        # Reset the environment\n",
    "        state = env.reset()[0] #shape(4,)\n",
    "        epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
    "        # Run one episode\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # # Choose and perform an action\n",
    "            CI_parameter= CI_end+CI_start-max(CI_end, CI_start * (CI_decay_rate ** episode))\n",
    "            # CI_parameter=0.2\n",
    "            action = new_agent.act_safe(state, model,CI_parameter,epsilon)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            cost=cal_cost(state,action,next_state)\n",
    "            buffer.append((state, action, reward-penalty*cost, next_state, done))\n",
    "            cost_episodes[episode]+=cost\n",
    "            if cost>safety_bound:\n",
    "                violation[episode]+=1\n",
    "            if len(buffer) >= batch_size:\n",
    "                batch = random.sample(buffer, batch_size)\n",
    "                new_agent.learn(batch, gamma) # Update the agent's knowledge\n",
    "            \n",
    "            #train BNN\n",
    "            datapoints=np.concatenate((state, np.array(action).reshape(1, )), axis=0)\n",
    "            data_tensor = torch.from_numpy(datapoints).float().unsqueeze(0).to(device)\n",
    "            labels=np.array(cost).reshape(1,)\n",
    "            labels_tensor=torch.from_numpy(labels).float().unsqueeze(0).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            mean, log_var = model(data_tensor)\n",
    "            loss = gaussian_nll_loss(mean, log_var, labels_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration[episode]+=1\n",
    "            ub= mean.item()+0.1*np.sqrt(np.exp(log_var.item()))\n",
    "            if ub>=safety_bound:\n",
    "                break\n",
    "            if violation[episode]>1.5:\n",
    "                break\n",
    "            state = next_state\n",
    "            # Check if the episode has ended\n",
    "            if done:\n",
    "                break\n",
    "    # Evaluate the agent's performance\n",
    "\n",
    "    episode_rewards = []\n",
    "    test_epi_length=np.zeros(test_episodes)\n",
    "    test_cost_epi=np.zeros(test_episodes)\n",
    "    test_vio_epi=np.zeros(test_episodes)\n",
    "    for episode in range(test_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        for step in range(max_steps_per_test_episode):\n",
    "            action = new_agent.act_safe(state, model,CI_para=0.6,eps=0)\n",
    "            # action = new_agent.act(state,eps=0)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            cost=cal_cost(state,action,next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            test_epi_length[episode]+=1\n",
    "            test_cost_epi[episode]+=cost\n",
    "            if cost>safety_bound:\n",
    "                test_vio_epi[episode]+=1\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    average_reward = sum(episode_rewards) / test_episodes\n",
    "    return average_reward,test_cost_epi,test_vio_epi,test_epi_length, new_agent,model,np.sum(violation)/np.sum(iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "793c3864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4000000000000001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode=2200\n",
    "CI_end+CI_start-max(CI_end, CI_start * (CI_decay_rate ** episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41aaad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_multi_rep=10\n",
    "mul_calls = [delayed(one_rep)() for rep in range(num_multi_rep)]\n",
    "result_mul_delayed = Parallel(n_jobs=-3)(mul_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd74d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_multi_rep=10\n",
    "agent_multi=[]\n",
    "model_multi=[]\n",
    "test_ave_reward_multi=np.zeros(num_multi_rep)\n",
    "test_cost_epi_multi=np.zeros((num_multi_rep,test_episodes))\n",
    "test_vio_epi_multi=np.zeros((num_multi_rep,test_episodes))\n",
    "test_epi_length_multi=np.zeros((num_multi_rep,test_episodes))\n",
    "train_viorate_epi_multi=np.zeros(num_multi_rep)\n",
    "for i in range(num_multi_rep):\n",
    "    test_ave_reward_multi[i]=np.copy(result_mul_delayed[i][0])\n",
    "    test_cost_epi_multi[i]=np.copy(result_mul_delayed[i][1])\n",
    "    test_vio_epi_multi[i]=np.copy(result_mul_delayed[i][2])\n",
    "    test_epi_length_multi[i]=np.copy(result_mul_delayed[i][3])\n",
    "    agent_multi.append(result_mul_delayed[i][4])\n",
    "    model_multi.append(result_mul_delayed[i][5])\n",
    "    train_viorate_epi_multi[i]=np.copy(result_mul_delayed[i][6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89f68f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199.853,\n",
       " 0.0001323916009803908,\n",
       " 1.5774782409135913e-07,\n",
       " array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00132392, 0.        , 0.        ]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vio_rate_multi=np.mean(test_vio_epi_multi/test_epi_length_multi,axis=1)\n",
    "np.mean(test_ave_reward_multi),np.mean(vio_rate_multi),np.var(vio_rate_multi),vio_rate_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e8e6690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011934400093376624, 0.0004035090363245207)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_viorate_epi_multi),np.std(train_viorate_epi_multi)/np.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e804eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 198.95\n",
      "Average reward over 100 test episodes: 200.00\n",
      "Average reward over 100 test episodes: 200.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(num_multi_rep):\n",
    "    test_ave_reward_multi[i],test_cost_epi_multi[i],test_vio_epi_multi[i],test_epi_length_multi[i]=evaluate(agent_multi[i],model_multi[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8027344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199.895,\n",
       " 0.09961174629530502,\n",
       " 0.00012988026247906861,\n",
       " 1.518199432346858e-07,\n",
       " array([0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.0012988, 0.       , 0.       ]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vio_rate_multi=np.mean(test_vio_epi_multi/test_epi_length_multi,axis=1)\n",
    "np.mean(test_ave_reward_multi),np.std(test_ave_reward_multi)/np.sqrt(10),np.mean(vio_rate_multi),np.var(vio_rate_multi),vio_rate_multi\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
